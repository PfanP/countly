'use strict';

module.exports = {
	EVENTS: {
		/** Sent by Child to Master in order to dispatch message to a particular Worker:
		 * {cmd: 'send', message: { ... }, credentials: [ ... ]}
		 */
		MASTER_SEND: 'master_send',

		/** Sent by Child to Master in order to abort message processing
		 * {cmd: 'abort', messageId: 'message ID'}
		 */
		MASTER_ABORT: 'master_abort',

		/** Sent by worker Child to Master in order to notify it about his progress in message processing
		 * {cmd: 'status', messageId: 'message ID', result: {status: Status.Aborted | Status.Processing | ..., streams: 1, sent: 123, total: 333[, warning: 'warning string'][, error: 'error string']}}
		 */
		MASTER_STATUS: 'master_status',

		/** Sent by Master to requesting Child to notify it about message status change (started, sent, aborted, partially sent)
		 * {cmd: 'update', messageId: 'message ID', result: {status: Status.Aborted | Status.Processing | ..., streams: 1, sent: 123, total: 333[, warning: 'warning string'][, error: 'error string']}}
		 */
		CHILD_STATUS: 'child_status',

		/** Sent by Master to sending Child in order to start sending message
		 * {cmd: 'process', message: { ... }, credentials: [ ... ]}
		 */
		CHILD_PROCESS: 'child_process',

		/** Sent by Master to sending Child in order to abort sending message
		 * {cmd: 'process', messageId: 'message ID', credentials: [ ... ]}
		 */
		CHILD_ABORT: 'child_abort',
	},
	SP: { 	// Events for internal use
		ERROR: 'pushly_internal_error',				// error occurred (see error.js)
		MESSAGE: 'pushly_internal_sent',			// notification is sent
		CLOSED: 'pushly_internal_closed',			// connection is closed
	},
	OPTIONS: {
		/** Send status update events each 1000 milliseconds */
		statusUpdatePeriod: 2000,
		/** Maintain (do not close immediately after sending) no more than 50 connections per worker */
		connections: 50,
		/** Keep no more than 100000 messages per connection in memory. Whenever sending is slower than messages stream,
		 * input stream will be throttled down (if using devicesQuery), or send method will just return false.
		 * Keep this option high, EXPERIMENTAL FEATURE.
		 */
		queue: 100000,
		/** Maintain (do not close immediately after sending) no more than 20 connections per worker per app.
		 * This effectively means that no more than 20 messages per app are being sent at any point in time.
		 * APNS part maintains 20 connections while GCM part sends 20 parallel requests with core keep-alive functionality.
		 */
		connectionsPerCredentials: 20,
		/** How much devices is required to open up additional connection within a worker (but no more than connectionsPerCredentials).
		 * 100 000 devices / 1000 = 100, but no more than 20 = 20 connections will be open when sending a message to 100 000 devices.
		 */
		connectionDivider: 10,
		/** 1 minute connection TTL */
		connectionTTL: 1000 * 60 * 100,
		/** 3 seconds to connect for a socket */
		connectionConnectTimeout: 1000 * 5,
		/** Part of worker picking algorithm: ( (hasConnection ? 0 : priceOfConnection) + priceOfQueue * queueLength ) */
		priceOfConnection: 100,
		/** Part of worker picking algorithm: ( (hasConnection ? 0 : priceOfConnection) + priceOfQueue * queueLength ) */
		priceOfQueue: 10,
		/** How much certificates to hold in memory instead of reading from file */
		apnsCertificatesCache: 50,
		/** How much APN notifications to transmit in a batch */
		apnsTransmitAtOnce: 10,
		/** How much GCM notifications to transmit in a batch for the same content */
		gcmTransmitAtOnce: 50,
		/** How much notifications to hold in memory in order to process error responses from APNS.
		 * APNS can respond with error to a particular notificaton with some delay. This cache ensures that notification
		 * which caused an error is still in memory so we could report about it to a sender.
		 * 100 Notifications are being held in memory per 1 APNS connection.
		 */
		apnsNotificationCacheForErrorPurposes: 1000,

		/**
		 * Start with this number of connections
		 */
		initialConnectionPool: 1, 				// connections

		/**
		 * How often to check CPU & memory in order to prevent 100% utilization
		 */
		profilerCheckPeriod: 1, 				// seconds
		/**
		 * When CPU reaches this utilization, pushly will decrease pushing rates.
		 * When CPU is underutilized, pushing rates can be increased if necessary.
		 */
		profilerMaxCPU: 0.8,
		/**
		 * How wide pushing rate smoothing period should be
		 */
		ratesSmoothingPeriod: 10,		 		// seconds
		/**
		 * How much faster messages should come compared to outgoing rate to grow connection pool in a worker
		 */
		ratesConnectionPoolGrowRatio: 1.01, 	// 101 msgs/second incoming   /   100 msgs/second processed
		/**
		 * How much slower messages should come compared to outgoing rate to shrink connection pool in a worker
		 */
		ratesConnectionPoolShrinkRatio: 0.5, 	// 50  msgs/second incoming   /   100 msgs/second processed
		/**
		 * How much time to wait between decisions to grow / shrink connection pool
		 */
		ratesConnectionPoolCooldown: 2, 		// check->grow [2 seconds] check->grow [2 seconds] check->nope [2 seconds] ... check->shrink [2 seconds] check->shrink [2 seconds]
		/**
		 * How much time to wait between decisions to grow / shrink connection pool
		 */
		threadBatchSize: 1000, 					// when connection has queue of size less than batch size * 3, it gets a batch size - sized refill from its cluster
	},
	debug: function(module){
		if (process.env.DEBUG) {
			try {
				return require('debug')('pushly:' + module);
			}
			catch (e) {
				console.log('Notice: "debug" module is not available. This should be installed with `npm install debug` to enable debug messages', e);
				return function() {};
			}
		} else {
			return function() {};
		}
	}
};
